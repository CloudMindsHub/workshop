+++
title = "Chu·∫©n b·ªã dataset"
date = 2025
weight = 2
chapter = false
pre = "<b>2. </b>"
+++


### T·ªïng quan v·ªÅ b·ªô d·ªØ li·ªáu

B·ªô d·ªØ li·ªáu n√†y l√† ph·∫ßn m·ªü r·ªông c·ªßa d·ª± √°n Viet-Doc VAQ, ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ 64.765 trang t√†i li·ªáu gi√°o d·ª•c ti·∫øng Vi·ªát üáªüá≥, bao g·ªìm: s√°ch b√†i t·∫≠p, s√°ch chuy√™n ƒë·ªÅ, gi√°o √°n t·ª´ c√°c nh√† xu·∫•t b·∫£n ch√≠nh th·ªëng nh∆∞ B·ªô Gi√°o d·ª•c & ƒê√†o t·∫°o, C√°nh Di·ªÅu, Ch√¢n Tr·ªùi S√°ng T·∫°o, K·∫øt N·ªëi Tri Th·ª©c,... ph·ªß r·ªông t·∫•t c·∫£ c√°c m√¥n h·ªçc t·ª´ l·ªõp 1 ƒë·∫øn l·ªõp 12.

M·ªói trang t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† ch√∫ th√≠ch k·ªπ l∆∞·ª°ng b·∫±ng c√°c k·ªπ thu·∫≠t Visual Question Answering (VQA), t·∫°o n√™n m·ªôt b·ªô d·ªØ li·ªáu c√≥ ch·∫•t l∆∞·ª£ng cao v√† gi√†u th√¥ng tin.

B·ªô d·ªØ li·ªáu bao g·ªìm 388.277 c·∫∑p m√¥ t·∫£ chi ti·∫øt v√† c√¢u h·ªèi - tr·∫£ l·ªùi theo ng·ªØ c·∫£nh, ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫°o ra b·ªüi Gemini 1.5 Flash ‚Äì m√¥ h√¨nh AI h√†ng ƒë·∫ßu c·ªßa Google, ƒëang d·∫´n ƒë·∫ßu b·∫£ng x·∫øp h·∫°ng WildVision Arena. Nh·ªù ƒë√≥, ƒë√¢y l√† ngu·ªìn t√†i nguy√™n l√Ω t∆∞·ªüng cho c√°c m·ª•c ti√™u nghi√™n c·ª©u v√† ·ª©ng d·ª•ng gi√°o d·ª•c hi·ªán ƒë·∫°i.

C√°c m√¥n h·ªçc bao g·ªìm: To√°n h·ªçc üìê, Ng·ªØ vƒÉn üìö, Ti·∫øng Anh üá¨üáß, V·∫≠t l√Ω ‚öõÔ∏è, H√≥a h·ªçc üß™, Sinh h·ªçc üå±, L·ªãch s·ª≠ üìú, ƒê·ªãa l√Ω üåç, Gi√°o d·ª•c c√¥ng d√¢n üè´, Tin h·ªçc üíª, C√¥ng ngh·ªá üõ†Ô∏è, √Çm nh·∫°c üéµ, M·ªπ thu·∫≠t üé®, Th·ªÉ d·ª•c ‚öΩ,...

V√¨ chi ph√≠ c√≥ gi·ªõi h·∫°n, ch·ªâ ƒë·ªÉ th·ª≠ nghi·ªám vi·ªác fine-tuning m√¥ h√¨nh tr√™n AWS Bedrock, trong workshop n√†y ch·ªâ d√πng 100 m·∫´u d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán (s·ªë l∆∞·ª£ng m·∫´u t·ªëi thi·ªÉu theo y√™u c·∫ßu c·ªßa Bedrock ). 

### Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu

- T·∫£i d·ªØ li·ªáu t·ª´ Hugging Face: https://huggingface.co/datasets/5CD-AI/Viet-Doc-VQA-II/tree/main/data

![Architecture](/images/2-dataset/data-hf.png)

- Nh·∫≠p th∆∞ vi·ªán c·∫ßn thi·∫øt

```
import os
import ast
import csv
import json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
```

- ƒê·ªçc file parquet th√†nh Dataframe

```
df = pd.read_parquet('train-00000-of-00034.parquet', engine="pyarrow")

df['conversations'] = df['conversations'].apply(
    lambda x: json.dumps(x.tolist(), ensure_ascii=False) if isinstance(x, np.ndarray) else json.dumps(x, ensure_ascii=False)
)

df = df.head(100)
```

- Chia d·ªØ li·ªáu th√†nh 3 b·ªô train (90%), test (10%)

```
# Chia t·∫≠p train (90%) v√† t·∫≠p test (10%)
train_df, test_df = train_test_split(df, test_size=0.1)

train_df.to_csv("viet_vqa_train.csv", index=False, encoding='utf-8')
test_df.to_csv("viet_vqa_test.csv", index=False, encoding='utf-8')

df_train = pd.read_csv('viet_vqa_train.csv')
df_test = pd.read_csv('viet_vqa_test.csv')
```

->>>>>>>>>>>>>>>>>1 t·∫•m h√¨nh ch·ªó n√†y show console s·ªë l∆∞·ª£ng d·ªØ li·ªáu


### ƒê·ªãnh d·∫°ng d·ªØ li·ªáu theo format AWS Bedrock

- ƒê·ªãnh d·∫°ng m·∫´u c·ªßa AWS Bedrock
```
item = {
    "schemaVersion": "bedrock-conversation-2024",
    "system": [
        {
            "text": (
                "You are an AI assistant specialized in Q&A based on Vietnamese images. "
                "You can analyze image content, recognize text (OCR), understand context, "
                "and answer questions based on extracted information. "
                "Behavior Guidelines Text Recognition: Use OCR to extract textual content from images. "
                "Context Understanding: Identify the meaning of the extracted text and image to provide accurate answers. "
                "Concise Responses: Provide clear, precise, and natural Vietnamese responses. "
                "Image Analysis Support: If a question requires more than text extraction, analyze the visual content as well. "
                "Professional & Objective: Respond factually based on the image, avoiding assumptions beyond the given data."
            )
        }
    ],
    "messages": [
        {
            "role": "user",
            "content": [
                {"text": ""},
                {
                    "image": {
                        "format": "png",
                        "source": {
                            "s3Location": {
                                "uri": "s3://test-fineture-novalite/data-fineture/image_sample_1.jpg",
                                "bucketOwner": "536*********"
                            }
                        }
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {"text": ""}
            ]
        }
    ]
}
```

- H√†m ƒë·ªãnh d·∫°ng d·ªØ li·ªáu, th√¥ng ƒëi√™p ƒë·∫ßu ti√™n c·ªßa role user s·∫Ω ch·ª©a th√™m h√¨nh ·∫£nh, k·∫øt h·ª£p v·ªõi c√°c th√¥ng ƒëi·ªáp trong m·∫´u d·ªØ li·ªáu t·∫°o ra 1 conversation.
```
def create_jsonl_item(row):
    try:
        conversations = row.get("conversations")
        json_conversation = json.loads(conversations)

        for item in json_conversation:
            if isinstance(item["content"], str):
                item["content"] = [{"text": item["content"]}]

        message_id = row.get("id")
        if isinstance(conversations, str):
            conversations = json.loads(conversations)
            
        if not isinstance(conversations, list):
            raise ValueError("Invalid JSON format in conversations column")

        if len(json_conversation) > 0 and json_conversation[0]["role"] == "user":
            image_info = {
                "image": {
                    "format": "png",
                    "source": {
                        "s3Location": {
                            "uri": f"s3://data-vqa-fine-tune-nova/train/image_{message_id}.png",
                            "bucketOwner": "536*********"
                        }
                    }
                }
            }
            content_user = json_conversation[0]["content"]
            content_user.append(image_info)
            json_conversation[0]["content"] = content_user
        print(json_conversation)
        return {
            "schemaVersion": "bedrock-conversation-2024",
            "system": [
                {
                    "text": (
                        "You are an AI assistant specialized in Q&A based on Vietnamese images. "
                        "You can analyze image content, recognize text (OCR), understand context, "
                        "and answer questions based on extracted information. "
                        "Behavior Guidelines Text Recognition: Use OCR to extract textual content from images. "
                        "Context Understanding: Identify the meaning of the extracted text and image to provide accurate answers. "
                        "Concise Responses: Provide clear, precise, and natural Vietnamese responses. "
                        "Image Analysis Support: If a question requires more than text extraction, analyze the visual content as well. "
                        "Professional & Objective: Respond factually based on the image, avoiding assumptions beyond the given data."
                    )
                }
            ],
            "messages": json_conversation
        }
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω conversations: {e}, row conversation: {conversations}")
        return None
```

- ƒê·ªçc d·ªØ li·ªáu train t·ª´ file **viet_vqa_train.csv**
```
df_train = pd.read_csv('viet_vqa_train.csv')
df_train
```

- Chuy·ªÉn ƒë·ªïi h√¨nh ·∫£nh d·∫°ng bytes th√†nh file png v√† l∆∞u trong m·ªôt folder ƒë·ªÉ upload l√™n s3 bucket.
```
output_folder = 'train_images'

if not os.path.exists(output_folder):
    os.makedirs(output_folder)

for idx, row in df_train.iterrows():
    try:
        image_bytes = eval(row['image'])
        image_data = image_bytes['bytes'] if isinstance(image_bytes, dict) else image_bytes

        img = Image.open(io.BytesIO(image_data))

        file_path = os.path.join(output_folder, f'image_{idx}.png')

        img.save(file_path, format='PNG')

        print(f"ƒê√£ l∆∞u ·∫£nh: {file_path}")
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω ·∫£nh ·ªü d√≤ng {idx}: {e}")
```

- Apply cho c·∫£ Dataframe
```
jsonl_data = df_train.apply(create_jsonl_item, axis=1).tolist()
```

- L∆∞u l·∫°i th√†nh file jsonl
```
jsonl_data = [item for item in jsonl_data if item is not None]

with open("data_train_90.jsonl", "w", encoding="utf-8") as f:
    for item in jsonl_data:
        json.dump(item, f, ensure_ascii=False)
        f.write("\n")
```

{{% notice note %}}
T∆∞∆°ng t·ª± nh∆∞ c√°ch l√†m c·ªßa b·ªô d·ªØ li·ªáu train, ta s·∫Ω √°p d·ª•ng cho file **viet_vqa_test.csv**.
{{% /notice %}}

- Upload t·∫•t c·∫£ l√™n s3. Ta ƒë∆∞·ª£c c·∫•u tr√∫c b·ªô d·ªØ li·ªáu tr√™n s3 nh∆∞ h√¨nh b√™n d∆∞·ªõi.

![S3-Dataset](/images/2-dataset/s3_dataset.png)
