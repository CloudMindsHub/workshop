+++
title = "Prepare dataset"
date = 2025
weight = 2
chapter = false
pre = "<b>2. </b>"
+++


### Dataset Overview

This dataset is an extension of the Viet Document VAQ project, built from **64,765 pages of Vietnamese 🇻🇳 educational documents**, including workbooks, thematic books, and lesson plans from reputable publishers such as the **Ministry of Education and Training (MOET)**, **Cánh Diều**, **Chân Trời Sáng Tạo**, and **Kết Nối Tri Thức**. It covers all subjects from **grades 1 to 12**.

Each page has been meticulously processed and annotated using advanced **Visual Question Answering (VQA)** techniques, resulting in a high-quality and information-rich dataset.

The dataset consists of **388,277 contextual question-answer pairs and detailed descriptions**, automatically generated by **Gemini 1.5 Flash** – Google’s leading AI model currently ranked #1 on the **WildVision Arena leaderboard**. This makes it an ideal resource for modern **educational applications and academic research**.

**Subjects included:** Mathematics 📐, Literature 📚, English 🇬🇧, Physics ⚛️, Chemistry 🧪, Biology 🌱, History 📜, Geography 🌍, Civics 🏫, Informatics 💻, Technology 🛠️, Music 🎵, Art 🎨, Physical Education ⚽, and more.

![Fine-tuning illustration](/images/fine-tune-example.png)

### Dataset Processing

- Import required libraries

```python
import json
import csv
import pandas as pd
import numpy as np
```

- Read parquet file into DataFrame

```python
df = pd.read_parquet('train-00000-of-00034.parquet', engine="pyarrow")

df['conversations'] = df['conversations'].apply(
    lambda x: json.dumps(x.tolist(), ensure_ascii=False) if isinstance(x, np.ndarray) else json.dumps(x, ensure_ascii=False)
)
```

- Split dataset into train (80%), val (10%), and test (10%)

```python
from sklearn.model_selection import train_test_split

# Split into training (80%) and val_test (20%)
train_df, val_test_df = train_test_split(df, test_size=0.2)

# Further split val_test into validation (10%) and test (10%)
val_df, test_df = train_test_split(val_test_df, test_size=0.5)

train_df.to_csv("viet_vqa_train.csv", index=False, encoding='utf-8')
val_df.to_csv("viet_vqa_val.csv", index=False, encoding='utf-8')
test_df.to_csv("viet_vqa_test.csv", index=False, encoding='utf-8')

df_train = pd.read_csv('viet_vqa_train.csv')
df_val = pd.read_csv('viet_vqa_val.csv')
df_test = pd.read_csv('viet_vqa_test.csv')
```

### Format Data According to AWS Bedrock Format

- Sample AWS Bedrock format

```json
{
  "schemaVersion": "bedrock-conversation-2024",
  "system": [
    {
      "text": "You are an AI assistant specialized in Q&A based on Vietnamese images. You can analyze image content, recognize text (OCR), understand context, and answer questions based on extracted information. Behavior Guidelines Text Recognition: Use OCR to extract textual content from images. Context Understanding: Identify the meaning of the extracted text and image to provide accurate answers. Concise Responses: Provide clear, precise, and natural Vietnamese responses. Image Analysis Support: If a question requires more than text extraction, analyze the visual content as well. Professional & Objective: Respond factually based on the image, avoiding assumptions beyond the given data."
    }
  ],
  "messages": [
    {
      "role": "user",
      "content": [
        {"text": ""},
        {
          "image": {
            "format": "png",
            "source": {
              "s3Location": {
                "uri": "s3://test-fineture-novalite/data-fineture/image_sample_1.jpg",
                "bucketOwner": "590******512"
              }
            }
          }
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {"text": ""}
      ]
    }
  ]
}
```

- Function to format data

```python
def create_jsonl_item(row):
    try:
        conversations = row.get("conversations")
        message_id = row.get("id")
        if isinstance(conversations, str):
            conversations = json.loads(conversations)

        if not isinstance(conversations, list):
            raise ValueError("Invalid JSON format in conversations column")

        if len(conversations) > 0 and conversations[0]["role"] == "user":
            original_text = conversations[0]["content"]
            image_info = {
                "image": {
                    "format": "png",
                    "source": {
                        "s3Location": {
                            "uri": f"s3://data-vqa-fine-tune-nova/train/image_{message_id}.png",
                            "bucketOwner": "536697245883"
                        }
                    }
                }
            }
            conversations[0]["content"] = [
                {"text": original_text},
                image_info
            ]

        return {
            "schemaVersion": "bedrock-conversation-2024",
            "system": [
                {
                    "text": (
                        "You are an AI assistant specialized in Q&A based on Vietnamese images. "
                        "You can analyze image content, recognize text (OCR), understand context, "
                        "and answer questions based on extracted information. "
                        "Behavior Guidelines Text Recognition: Use OCR to extract textual content from images. "
                        "Context Understanding: Identify the meaning of the extracted text and image to provide accurate answers. "
                        "Concise Responses: Provide clear, precise, and natural Vietnamese responses. "
                        "Image Analysis Support: If a question requires more than text extraction, analyze the visual content as well. "
                        "Professional & Objective: Respond factually based on the image, avoiding assumptions beyond the given data."
                    )
                }
            ],
            "messages": conversations
        }
    except Exception as e:
        print(f"Error processing conversations: {e}, row ID: {row['id']}, {conversations}")
        return None
```

- Apply function to entire DataFrame

```python
jsonl_data = df_sample.apply(create_jsonl_item, axis=1).tolist()
```

- Save to JSONL file

```
jsonl_data = [item for item in jsonl_data if item is not None]

with open("data_finetune_100.jsonl", "w", encoding="utf-8") as f:
    for item in jsonl_data:
        json.dump(item, f, ensure_ascii=False)
        f.write("\n")
```

{{% notice note %}}
 Follow the same process to generate JSONL files for the validation and test datasets.
{{% /notice %}}
